var documenterSearchIndex = {"docs":
[{"location":"understanding_intro/#Tapir.jl-and-Reverse-Mode-AD","page":"Introduction","title":"Tapir.jl and Reverse-Mode AD","text":"","category":"section"},{"location":"understanding_intro/","page":"Introduction","title":"Introduction","text":"The point of Tapir.jl is to perform reverse-mode algorithmic differentiation (AD). The purpose of this section is to explain what precisely is meant by this, and how it can be interpreted mathematically.","category":"page"},{"location":"understanding_intro/","page":"Introduction","title":"Introduction","text":"we recap what AD is, and introduce the mathematics necessary to understand is,\nexplain how this mathematics relates to functions and data structures in Julia, and\nhow this is handled in Tapir.jl.","category":"page"},{"location":"understanding_intro/","page":"Introduction","title":"Introduction","text":"Since Tapir.jl supports in-place operations / mutation, these will push beyond what is encountered in Zygote / Diffractor / ChainRules. Consequently, while there is a great deal of overlap with these existing systems, you will need to read through this section of the docs in order to properly understand Tapir.jl.","category":"page"},{"location":"understanding_intro/#Who-Are-These-Docs-For?","page":"Introduction","title":"Who Are These Docs For?","text":"","category":"section"},{"location":"understanding_intro/","page":"Introduction","title":"Introduction","text":"These are primarily designed for anyone who is interested in contributing to Tapir.jl. They are also hopefully of interest to anyone how is interested in understanding AD more broadly. If you aren't interested in understanding how Tapir.jl and AD work, you don't need to have read them in order to make use of this package.","category":"page"},{"location":"understanding_intro/#Prerequisites-and-Resources","page":"Introduction","title":"Prerequisites and Resources","text":"","category":"section"},{"location":"understanding_intro/","page":"Introduction","title":"Introduction","text":"This introduction assumes familiarity with the differentiation of vector-valued functions – familiarity with the gradient and Jacobian matrices is a given.","category":"page"},{"location":"understanding_intro/","page":"Introduction","title":"Introduction","text":"In order to provide a convenient exposition of AD, we need to abstract a little further than this and make use of a slightly more general notion of the derivative, gradient, and \"transposed Jacobian\". Please note that, fortunately, we only ever have to handle finite dimensional objects when doing AD, so there is no need for any knowledge of functional analysis to understand what is going on here. The required concepts will be introduced here, but I cannot promise that these docs give the best exposition – they're most appropriate as a refresher and to establish notation. Rather, I would recommend a couple of lectures from the \"Matrix Calculus for Machine Learning and Beyond\" course, which you can find on MIT's OCW website, delivered by Edelman and Johnson (who will be familiar faces to anyone who has spent much time in the Julia world!). It is designed for undergraduates, and is accessible to anyone with some undergraduate-level linear algebra and calculus. While I recommend the whole course, Lecture 1 part 2 and Lecture 4 part 1 are especially relevant to the problems we shall discuss – you can skip to 11:30 in Lecture 4 part 1 if you're in a hurry.","category":"page"},{"location":"single_block_rmad/#Reverse-Mode-AD:-*how*-does-it-do-it?","page":"AD Without Control Flow","title":"Reverse-Mode AD: how does it do it?","text":"","category":"section"},{"location":"single_block_rmad/","page":"AD Without Control Flow","title":"AD Without Control Flow","text":"As discussed in Reverse-Mode AD: what does it do? the purpose of reverse-mode AD is to apply the adjoint of the derivative, D f (x)^ast to a vector bary.","category":"page"},{"location":"single_block_rmad/","page":"AD Without Control Flow","title":"AD Without Control Flow","text":"In principle, AD achieves this by decomposing f into the composition f = f_N circ dots circ f_1, we assume that we can compute the adjoint of the derivative of each f_n. Then the adjoint is","category":"page"},{"location":"single_block_rmad/","page":"AD Without Control Flow","title":"AD Without Control Flow","text":"D f x^ast (bary) = (D f_1 x_1^ast circ dots circ D f_N x_N^ast)(bary)","category":"page"},{"location":"single_block_rmad/","page":"AD Without Control Flow","title":"AD Without Control Flow","text":"Reverse-mode AD is performed (roughly speaking) as follows.","category":"page"},{"location":"single_block_rmad/","page":"AD Without Control Flow","title":"AD Without Control Flow","text":"Forwards-Pass:","category":"page"},{"location":"single_block_rmad/","page":"AD Without Control Flow","title":"AD Without Control Flow","text":"x_1 = x, n = 1\nconstruct D f_n x_n^ast\nlet x_n+1 = f_n (x_n)\nlet n = n + 1\nif n  N + 1 then go to 2","category":"page"},{"location":"single_block_rmad/","page":"AD Without Control Flow","title":"AD Without Control Flow","text":"Reverse-Pass:","category":"page"},{"location":"single_block_rmad/","page":"AD Without Control Flow","title":"AD Without Control Flow","text":"let barx_N+1 = bary\nlet n = n - 1\nlet barx_n = D f_n x_n^ast (barx_n+1)\nif n = 1 return barx_1 else go to 2.","category":"page"},{"location":"single_block_rmad/","page":"AD Without Control Flow","title":"AD Without Control Flow","text":"How does this relate to vector-Jacobian products?","category":"page"},{"location":"single_block_rmad/","page":"AD Without Control Flow","title":"AD Without Control Flow","text":"In Euclidean space we have the collection of Jacobians J_nx_n. By the chain rule","category":"page"},{"location":"single_block_rmad/","page":"AD Without Control Flow","title":"AD Without Control Flow","text":"Jx = J_Nx_N dots J_1x_1 ","category":"page"},{"location":"single_block_rmad/","page":"AD Without Control Flow","title":"AD Without Control Flow","text":"Taking the transpose and multiplying from the left by bary yields","category":"page"},{"location":"single_block_rmad/","page":"AD Without Control Flow","title":"AD Without Control Flow","text":"Jx^top bary = Jx_N^top_N dots Jx_1^top_1 bary ","category":"page"},{"location":"single_block_rmad/","page":"AD Without Control Flow","title":"AD Without Control Flow","text":"Comparing this with the expression in terms of adjoints and operators, we see that composition of adjoints of derivatives has been replaced with multiplying by transposed Jacobian matrices. This expression is likely familiar to many readers.","category":"page"},{"location":"algorithmic_differentiation/#Algorithmic-Differentiation","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"","category":"section"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"This section introduces the mathematics behind AD. Even if you have worked with AD before, we recommend reading in order to acclimatise yourself to the perspective that Tapir.jl takes on the subject.","category":"page"},{"location":"algorithmic_differentiation/#Derivatives","page":"Algorithmic Differentiation","title":"Derivatives","text":"","category":"section"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"A foundation on which all of AD is built the the derivate – we require a fairly general definition of it, which we build up to here.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Scalar-to-Scalar Functions","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Consider first f  RR to RR, which we require to be differentiable at x in RR. Its derivative at x is usually thought of as the scalar alpha in RR such that","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"textdf = alpha  textdx ","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Loosely speaking, by this notation we mean that for arbitrary small changes textd x in the input to f, the change in the output textd f is alpha  textdx. We refer readers to the first few minutes of the first lecture mentioned above for a more careful explanation.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Vector-to-Vector Functions","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"The generalisation of this to Euclidean space should be familiar: if f  RR^P to RR^Q is differentiable at a point x in RR^P, then the derivative of f at x is given by the Jacobian matrix at x, denoted Jx in RR^Q times P, such that","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"textdf = Jx  textdx ","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"It is possible to stop here, as all the functions we shall need to consider can in principle be written as functions on some subset RR^P.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"However, when we consider differentiating computer programmes, we will have to deal with complicated nested data structures, e.g. structs inside Tuples inside Vectors etc. While all of these data structures can be mapped onto a flat vector in order to make sense of the Jacobian of a computer programme, this becomes very inconvenient very quickly. To see the problem, consider the Julia function whose input is of type Tuple{Tuple{Float64, Vector{Float64}}, Vector{Float64}, Float64} and whose output is of type Tuple{Vector{Float64}, Float64}. What kind of object might be use to represent the derivative of a function mapping between these two spaces? We certainly can treat these as structured \"view\" into a \"flat\" Vector{Float64}s, and then define a Jacobian, but actually finding this mapping is a tedious exercise even if it quite obviously exists.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Similarly, while \"vector-Jacobian\" products are usually used to explain reverse-mode AD, a more general formulation of the derivative is used all the time – the matrix calculus discussed by [1] and [2] (to name a couple) make use of a generalised form of derivative in order to work with functions which map to and from matrices (despite slight differences in naming conventions from text to text).","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Consequently, it will be much easier to avoid these kinds of \"flattening\" operations wherever possible. In order to do so, we make use of a generalised notion of the derivative.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Functions Between More General Spaces","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"In order to avoid the difficulties described above, we consider we consider functions f  mathcalX to mathcalY, where mathcalX and mathcalY are finite dimensional real Hilbert spaces (read: finite-dimensional vector space with an inner product, and real-valued scalars). This definition includes functions to / from RR, RR^D, but also real-valued matrices. Furthermore, we shall see later how we can model all sorts of structured representations of data directly as such spaces.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"For such spaces, the derivative of f at x in mathcalX is the linear operator (read: linear function) D f x  mathcalX to mathcalY satisfying","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"textdf = D f x  textd x","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"That is, instead of thinking of the derivative as a number or a matrix, we think about it as a function. We can express the previous notions of the derivative in this language.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"In the scalar case, rather than thinking of the derivative as being alpha, we think of it is a the linear operator D f x (dotx) = alpha dotx. Put differently, rather than thinking of the derivative as the slope of the tangent to f at x, think of it as the function decribing the tangent itself.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Similarly, if mathcalX = RR^P and mathcalY = RR^Q then this operator can be specified in terms of the Jacobian matrix: D f x (dotx) = Jx dotx – brackets are used to emphasise that D f x is a function, and is being applied to dotx.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"The difference from usual is a little bit subtle. We do not define the derivative to be alpha or Jx, rather we define it to be \"multiply by alpha\" or \"multiply by Jx\". For the rest of this document we shall use this definition of the derivative. So whenever you see the word \"derivative\", you should think \"linear function\".","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"The Chain Rule","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"The chain rule is the result which makes AD work. Fortunately, it applies to this version of the derivative:","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"f = g circ h implies D f x = (D g h(x)) circ (D h x)","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"By induction this extends to a collection of N functions f_1 dots f_N:","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"f = f_N circ dots circ f_1 implies D f x = (D f_N x_N) circ dots circ (D f_1 x_1)","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"where x_n+1 = f(x_n), and x_1 = x.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"An aside: the definition of the Frechet Derivative","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"This definition of the derivative has a name: the Frechet derivative. It is a generalisation of the Total Derivative. Formally, we say that a function f  mathcalX to mathcalY is differentiable at a point x in mathcalX if there exists a linear operator D f x  mathcalX to mathcalY (the derivative) satisfying","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"lim_textd h to 0 frac f(x + textd h) - f(x) + D f x (textd h)  _mathcalY textdh _mathcalX = 0","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"where  cdot _mathcalX and  cdot _mathcalY are the norms associated to Hilbert spaces mathcalX and mathcalY respectively. It is a good idea to consider what this looks like when mathcalX = mathcalY = RR and when mathcalX = mathcalY = RR^D. It is sometimes helpful to refer to this definition to e.g. verify the correctness of the derivative of a function – as with single-variable calculus, however, this is rare.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Another aside: what does Forwards-Mode AD compute?","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"At this point we have enough machinery to discuss forwards-mode AD. Expressed in the language of linear operators and Hilbert spaces, the goal of forwards-mode AD is the following: given a function f which is differentiable at a point x, compute D f x (dotx) for a given vector dotx. If f  RR^P to RR^Q, this is equivalent to computing Jx dotx, where Jx is the Jacobian of f at x. For the interested reader we provide a high-level explanation of how forwards-mode AD does this in How does Forwards-Mode AD work?.","category":"page"},{"location":"algorithmic_differentiation/#Reverse-Mode-AD:-*what*-does-it-do?","page":"Algorithmic Differentiation","title":"Reverse-Mode AD: what does it do?","text":"","category":"section"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"In order to explain what reverse-mode AD does, we first consider the \"vector-Jacobian product\" definition in Euclidean space which will be familiar to many readers. We then generalise.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Reverse-Mode AD: what does it do in Euclidean space?","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"In this setting, the goal of reverse-mode AD is the following: given a function f  RR^P to RR^Q which is differentiable at x in RR^P with Jacobian Jx at x, compute Jx^top bary for any bary in RR^Q. This is useful because we can obtain the gradient from this when Q = 1 by letting bary = 1.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Adjoint Operators","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"In order to generalise this algorithm to work with linear operators, we must first generalise the idea of multiplying a vector by the transpose of the Jacobian. The relevant concept here is that of the adjoint operator. Specifically, the adjoint A^ast of linear operator A is the linear operator satisfying","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"langle A^ast bary dotx rangle = langle bary A dotx rangle","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"The relationship between the adjoint and matrix transpose is this: if A (x) = J x for some matrix J, then A^ast (y) = J^top y.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Moreover, just as (A B)^top = B^top A^top when A and B are matrices, (A B)^ast = B^ast A^ast when A and B are linear operators. This result follows in short order from the definition of the adjoint operator – (and is a good exercise!)","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Reverse-Mode AD: what does it do in general?","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Equipped with adjoints, we can express reverse-mode AD only in terms of linear operators, dispensing with the need to express everything in terms of Jacobians. The goal of reverse-mode AD is as follows: given a differentiable function f  mathcalX to mathcalY, compute D f x^ast (bary) for some bary.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"We will explain how reverse-mode AD goes about computing this after some worked examples.","category":"page"},{"location":"algorithmic_differentiation/#Some-Worked-Examples","page":"Algorithmic Differentiation","title":"Some Worked Examples","text":"","category":"section"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"We now present some worked examples in order to prime intuition, and to introduce the important classes of problems that will be encountered when doing AD in the Julia language. We will put all of these problems in a single general framework later on.","category":"page"},{"location":"algorithmic_differentiation/#An-Example-with-Matrix-Calculus","page":"Algorithmic Differentiation","title":"An Example with Matrix Calculus","text":"","category":"section"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"We have introduced some mathematical abstraction in order to simplify the calculations involved in AD. To this end, we consider differentiating f(X) = X^top X. Results for this and similar operations are given by [1]. A similar operation, but which maps from matrices to RR is discussed in Lecture 4 part 2 of the MIT course mentioned previouly. Both [1] and Lecture 4 part 2 provide approaches to obtaining the derivative of this function.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Following either resource will yield the derivative:","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"D f X (dotX) = dotX^top X + X^top dotX","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Observe that this is indeed a linear operator (i.e. it is linear in its argument, dotX). (You can always plug it in to the definition of the Frechet derivative to confirm that it is indeed the derivative.)","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"In order to perform reverse-mode AD, we need to find the adjoint operator. Using the usual definition of the inner product between matrices,","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"langle X Y rangle = textrmtr (X^top Y)","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"we can rearrange the inner product as follows:","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"beginalign\n    langle barY D f X (dotX) rangle = langle barY dotX^top X + X^top dotX rangle nonumber \n        = textrmtr (barY^top dotX^top X) + textrmtr(barY^top X^top dotX) nonumber \n        = textrmtr ( barY X^top^top dotX) + textrmtr( X barY^top dotX) nonumber \n        = langle barY X^top + X barY dotX rangle nonumber\nendalign","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"We can read off the adjoint operator from the first argument to the inner product:","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"D f X^ast (barY) = barY X^top + X barY","category":"page"},{"location":"algorithmic_differentiation/#AD-of-a-Julia-function:-a-trivial-example","page":"Algorithmic Differentiation","title":"AD of a Julia function: a trivial example","text":"","category":"section"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"We now turn to differentiating Julia functions. The way that Tapir.jl handles immutable data is very similar to how Zygote / ChainRules do. For example, consider the Julia function","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"f(x::Float64) = sin(x)","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"If you've previously worked with ChainRules / Zygote, without thinking too hard about the formalisms we introduced previously (perhaps by considering a variety of partial derivatives) you can probably arrive at the following adjoint for the derivative of f:","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"g -> g * cos(x)","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Implicitly, you have performed three steps:","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"model f as a differentiable function,\ncompute its derivative, and\ncompute the adjoint of the derivative.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"It is helpful to work through this simple example in detail, as the steps involved apply more generally. The goal is to spell out the steps involved in detail, as this detail becomes helpful in more complicated examples. If at any point this exercise feels pedantic, we ask you to stick with it.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Step 1: Differentiable Mathematical Model","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Obviously, we model the Julia function f as the function f  RR to RR where","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"f(x) = sin(x)","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Observe that, we've made (at least) two modelling assumptions here:","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"a Float64 is modelled as a real number,\nthe Julia function sin is modelled as the usual mathematical function sin.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"As promised we're being quite pedantic. While the first assumption is obvious and will remain true, we will shortly see examples where we have to work a bit harder to obtain a correspondence between a Julia function and a mathematical object.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Step 2: Compute Derivative","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Now that we have a mathematical model, we can differentiate it:","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"D f x (dotx) = cos(x) dotx","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Step 3: Compute Adjoint of Derivative","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Given the derivative, we can find its adjoint:","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"langle barf D f x(dotx) rangle = langle barf cos(x) dotx rangle = langle cos(x) barf dotx rangle","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"From here the adjoint can be read off from the first argument to the inner product:","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"D f x^ast (barf) = cos(x) barf","category":"page"},{"location":"algorithmic_differentiation/#AD-of-a-Julia-function:-a-slightly-less-trivial-example","page":"Algorithmic Differentiation","title":"AD of a Julia function: a slightly less trivial example","text":"","category":"section"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Now consider the Julia function","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"f(x::Float64, y::Tuple{Float64, Float64}) = x + y[1] * y[2]","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Its adjoint is going to be something along the lines of","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"g -> (g, (y[2] * g, y[1] * g))","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"As before, we work through in detail.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Step 1: Differentiable Mathematical Model","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"There are a couple of aspects of f which require thought:","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"it has two arguments – we've only handled single argument functions previously, and\nthe second argument is a Tuple – we've not yet decided how to model this.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"To this end, we define a mathematical notion of a tuple. A tuple is a collection of N elements, each of which is drawn from some set mathcalX_n. We denote by mathcalX =  mathcalX_1 times dots times mathcalX_N  the set of all N-tuples whose nth element is drawn from mathcalX_n. Provided that each mathcalX_n forms a finite Hilbert space, mathcalX forms a Hilbert space with","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"alpha x = (alpha x_1 dots alpha x_N),\nx + y = (x_1 + y_1 dots x_N + y_N), and\nlangle x y rangle = sum_n=1^N langle x_n y_n rangle.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"We can think of multi-argument functions as single-argument functions of a tuple, so a reasonable mathematical model for f might be a function f   RR times  RR times RR   to RR, where","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"f(x y) = x + y_1 y_2","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Note that while the function is written with two arguments, you should treat them as a single tuple, where we've assigned the name x to the first element, and y to the second.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Step 2: Compute Derivative","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Now that we have a mathematical object, we can differentiate it:","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"D f x y(dotx doty) = dotx + doty_1 y_2 + y_1 doty_2","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Step 3: Compute Adjoint of Derivative","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"D fx y maps  RR times  RR times RR  to RR, so D f x y^ast must map the other way. You should verify that the following follows quickly from the definition of the adjoint:","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"D f x y^ast (barf) =  (barf (barf y_2 barf y_1))","category":"page"},{"location":"algorithmic_differentiation/#AD-with-mutable-data","page":"Algorithmic Differentiation","title":"AD with mutable data","text":"","category":"section"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"In the previous two examples there was an obvious mathematical model for the Julia function. Indeed this model was sufficiently obvious that it required little explanation. This is not always the case though, in particular, Julia functions which modify / mutate their inputs require a little more thought.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Consider the following Julia function:","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"function f!(x::Vector{Float64})\n    x .*= x\n    return sum(x)\nend","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"This function squares each element of its input in-place, and returns the sum of the result. So what is an appropriate mathematical model for this function?","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Step 1: Differentiable Mathematical Model","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"The trick is to distingush between the state of x upon entry to / exit from f!. In particular, let phi_textf  RR^N to  RR^N times RR  be given by","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"phi_textf(x) = (x odot x sum_n=1^N x_n^2)","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"where odot denotes the Hadamard / elementwise product. The point here is that the inputs to phi_textf are the inputs to x upon entry to f!, and the value returned from phi_textf is a tuple containing the both the inputs upon exit from f! and the value returned by f!.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"The remaining steps are straightforward now that we have the model.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Step 2: Compute Derivative","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"The derivative of phi_textf is","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"D phi_textf x(dotx) = (2 x odot x 2 sum_n=1^N x_n dotx_n)","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Step 3: Compute Adjoint of Derivative","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"The argument to the adjoint of the derivative must be a 2-tuple whose elements are drawn from RR^N times RR . Denote such a tuple as (bary_1 bary_2). Plugging this into an inner product with the derivative and rearranging yields","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"beginalign\n    langle (bary_1 bary_2) D phi_textf x (dotx) rangle = langle (bary_1 bary_2) (2 x odot dotx 2 sum_n=1^N x_n dotx_n) rangle nonumber \n        = langle bary_1 2 x odot dotx rangle + langle bary_2 2 sum_n=1^N x_n dotx_n rangle nonumber \n        = langle 2x odot bary_1 dotx rangle + langle 2 bary_2 x dotx rangle nonumber \n        = langle 2 (x odot bary_1 + bary_2 x) dotx rangle nonumber\nendalign","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"So we can read off the adjoint to be","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"D phi_textf x^ast (bary) = 2 (x odot bary_1 + bary_2 x)","category":"page"},{"location":"algorithmic_differentiation/#Reverse-Mode-AD:-*how*-does-it-do-it?","page":"Algorithmic Differentiation","title":"Reverse-Mode AD: how does it do it?","text":"","category":"section"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Now that we know what it is that AD computes, we need a rough understanding of how it computes it.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"In short: reverse-mode AD breaks down a \"complicated\" function f into the composition of a collection of \"simple\" functions f_1 dots f_N, applies the chain rule, and takes the adjoint.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Specifically, we assume that we can express any function f as f = f_N circ dots circ f_1, and that we can compute the adjoint of the derivative for each f_n. From this, we can obtain the adjoint of f by applying the chain rule to the derivatives and taking the adjoint:","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"beginalign\nD f x^ast = (D f_N x_N circ dots circ D f_1 x_1)^ast nonumber \n    = D f_1 x_1^ast circ dots circ D f_N x_N^ast nonumber\nendalign","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"For example, suppose that f(x) = sin(cos(texttr(X^top X))). One option to compute its adjoint is to figure it out by hand directly (probably using the chain rule somewhere). Instead, we could notice that f = f_4 circ f_3 circ f_2 circ f_1 where f_4 = sin, f_3 = cos, f_2 = texttr and f_1(X) = X^top X. We could derive the adjoint for each of these functions (a fairly straightforward task), and then compute","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"D f x^ast (bary) = (D f_1 x_1^ast circ D f_2 x_2^ast circ D f_3 x_3^ast circ D f_4 x_4^ast)(1)","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"in order to obtain the gradient of f. Reverse-mode AD essentially just does this. Modern systems have hand-written adjoints for (hopefully!) all of the \"simple\" functions you may wish to build a function such as f from (often there are hundreds of these), and composes them to compute the adjoint of f. A sketch of a more generic algorithm is as follows.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Forwards-Pass:","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"x_1 = x, n = 1\nconstruct D f_n x_n^ast\nlet x_n+1 = f_n (x_n)\nlet n = n + 1\nif n  N + 1 then go to 2","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Reverse-Pass:","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"let barx_N+1 = bary\nlet n = n - 1\nlet barx_n = D f_n x_n^ast (barx_n+1)\nif n = 1 return barx_1 else go to 2.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"How does this relate to vector-Jacobian products?","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"In Euclidean space, each derivative D f_n x_n(dotx_n) = J_nx_n dotx_n. Applying the chain rule to D f x and substituting this in yields","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Jx = J_Nx_N dots J_1x_1 ","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Taking the transpose and multiplying from the left by bary yields","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Jx^top bary = Jx_1^top_1 dots Jx_N^top_N bary ","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Comparing this with the expression in terms of adjoints and operators, we see that composition of adjoints of derivatives has been replaced with multiplying by transposed Jacobian matrices. This \"vector-Jacobian product\" expression is commonly used to explain AD, and is likely familiar to many readers.","category":"page"},{"location":"algorithmic_differentiation/#Directional-Derivatives-and-Gradients","page":"Algorithmic Differentiation","title":"Directional Derivatives and Gradients","text":"","category":"section"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Now we turn to using reverse-mode AD to compute the gradient of a function. In short, given a function g  mathcalX to RR with derivative D g x at x, its gradient is equal to D g x^ast (1). We explain why in this section.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"The derivative discussed here can be used to compute directional derivatives. Consider a function f  mathcalX to RR with Frechet derivative D f x  mathcalX to RR at x in mathcalX. Then D fx(dotx) returns the directional derivative in direction dotx.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Gradients are closely related to the adjoint of the derivative. Recall that the gradient of f at x is defined to be the vector nabla f (x) in mathcalX such that langle nabla f (x) dotx rangle gives the directional derivative of f at x in direction dotx. Having noted that D fx(dotx) is exactly this directional derivative, we can equivalently say that","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"D fx(dotx) = langle nabla f (x) dotx rangle ","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"The role of the adjoint is revealed when we consider f = mathcall circ g, where g  mathcalX to mathcalY, mathcall(y) = langle bary y rangle, and bary in mathcalY is some fixed vector. Noting that D mathcall y(doty) = langle bary doty rangle, we apply the chain rule to obtain","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"beginalign\nD f x (dotx) = (D mathcall g(x)) circ (D g x)(dotx) nonumber \n    = langle bary D g x (dotx) rangle nonumber \n    = langle D g x^ast (bary) dotx rangle nonumber\nendalign","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"from which we conclude that D g x^ast (bary) is the gradient of the composition l circ g at x.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"The consequence is that we can always view the computation performed by reverse-mode AD as computing the gradient of the composition of the function in question and an inner product with the argument to the adjoint.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"The above shows that if Y = RR and g is the function we wish to compute the gradient of, we can simply set bary = 1 and compute D g x^ast (bary) to obtain the gradient of g at x.","category":"page"},{"location":"algorithmic_differentiation/#Summary","page":"Algorithmic Differentiation","title":"Summary","text":"","category":"section"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"This document explains the core mathematical foundations of AD. It explains separately what is does, and how it goes about it. Some basic examples are given which show how these mathematical foundations can be applied to differentiate functions of matrices, and Julia functions.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Subsequent sections will build on these foundations, to provide a more general explanation of what AD looks like for a Julia programme.","category":"page"},{"location":"algorithmic_differentiation/#Asides","page":"Algorithmic Differentiation","title":"Asides","text":"","category":"section"},{"location":"algorithmic_differentiation/#*How*-does-Forwards-Mode-AD-work?","page":"Algorithmic Differentiation","title":"How does Forwards-Mode AD work?","text":"","category":"section"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"Forwards-mode AD achieves this by breaking down f into the composition f = f_N circ dots circ f_1, # where each f_n is a simple function whose derivative (function) D f_n x_n we know for any given x_n. By the chain rule, we have that","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"D f x (dotx) = D f_N x_N circ dots circ D f_1 x_1 (dotx)","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"which suggests the following algorithm:","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"let x_1 = x, dotx_1 = dotx, and n = 1\nlet dotx_n+1 = D f_n x_n (dotx_n)\nlet x_n+1 = f(x_n)\nlet n = n + 1\nif n = N+1 then return dotx_N+1, otherwise go to 2.","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"When each function f_n maps between Euclidean spaces, the applications of derivatives D f_n x_n (dotx_n) are given by J_n dotx_n where J_n is the Jacobian of f_n at x_n.v","category":"page"},{"location":"algorithmic_differentiation/","page":"Algorithmic Differentiation","title":"Algorithmic Differentiation","text":"M. Giles. An extended collection of matrix derivative results for forward and reverse mode automatic differentiation. Unpublished (2008).\n\n\n\nT. P. Minka. Old and new matrix algebra useful for statistics. See www. stat. cmu. edu/minka/papers/matrix. html 4 (2000).\n\n\n\nB. A. Pearlmutter and J. M. Siskind. Reverse-mode AD in a functional framework: Lambda the ultimate backpropagator. ACM Transactions on Programming Languages and Systems (TOPLAS) 30, 1–36 (2008).\n\n\n\n","category":"page"},{"location":"#Tapir.jl","page":"Tapir.jl","title":"Tapir.jl","text":"","category":"section"},{"location":"","page":"Tapir.jl","title":"Tapir.jl","text":"Documentation for Tapir.jl is on it's way!","category":"page"},{"location":"","page":"Tapir.jl","title":"Tapir.jl","text":"Note (29/05/2024): I (Will) am currently actively working on the documentation. It will be merged in chunks over the next month or so as good first drafts of sections are completed. Please don't be alarmed that not all of it is here!","category":"page"},{"location":"mathematical_interpretation/#Tapir.jl's-Mathematical-Intepretation-of-Julia-Functions","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Intepretation of Julia Functions","text":"","category":"section"},{"location":"mathematical_interpretation/#A-Mathematical-Model-for-a-Computer-Programme","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"A Mathematical Model for a Computer Programme","text":"","category":"section"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"In order to make sense of what it might mean to differentiate a computer programme, we need some kind of differentiable mathematical model for said programme. Pearlmutter [3] introduced such a model, which we expand on here.","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"Think of a computer as a device which has a state, which we model as a vector in RR^D. When a programme p is run, it changes this state. We model the programme p with a \"transition function\" t  RR^D to RR^D, whose input is whatever the state is before running p, and whose output is whatever the state is after running p. If t is differentiable, then we can meaningfully define \"differentiating p\" to mean \"differentiating t\". In particular, when we talk about applying reverse-mode AD to a particular Julia function p, what we mean is computing the adjoint D t x of the transition function t which describes p when the computer is in state x prior to running p.","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"Consider how we might model the Julia function","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"function f(x)\n    x1 = x\n    x2 = f1(x1)\n    x3 = f2(x1, x2)\n    ...\n    return fN(x1, x2, ..., xN)\nend","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"It can be thought of as a sequence of N programmes, which are run one after the other. If each fn is associated to transition functin t_n  RR^D to RR^D then the transtion function associated to f is just t = t_N circ dots circ t_1. We can then compute the adjoint of t in the way described in the last section:","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"D t x^ast (bary) =  D t_1 x_1 circ dots circ D t_N x_N ^ast (bary)","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"A Simple Worked Example","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"Consider applying this framework to the Julia function","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"f(x::Float64) = sin(cos(x))","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"We first find the transition operator associated to the primitives sin(::Float64) and cos(::Float64), then look at ","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"Advantages and Limitations","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"In principle, we have now associated a precise meaning to applying reverse-mode AD to the Julia function f and shown how reverse-mode AD might be applied. This view has several advantages. It places sufficiently few restrictions on what a computer programme is allowed to do that we can imagine it being a good model for most computer programmes we shall encounter. Moreover, it gives us a precise meaning to \"differentiating a computer programme\".","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"However, it has returned us to working with \"flat\" vector structures, rather than directly with the ","category":"page"},{"location":"mathematical_interpretation/#Tangents","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tangents","text":"","category":"section"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"We call the argument or output of a derivative D f x  mathcalX to mathcalY a tangent, and will usually denote it with a dot over a symbol, e.g. dotx. Conversely, we call an argument or output of the adjoint of this derivative D f x^ast  mathcalY to mathcalX a gradient, and will usually denote it with a bar over a symbol, e.g. bary.","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"Note, however, that the sets involved are the same whether dealing with a derivative or its adjoint. Consequently, we use the same type to represent both.","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"A quick aside: Non-Differentiable Data","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"In the introduction to algorithmic differentiation, we assumed that the domain / range of function are the same as that of its derivative. Unfortunately, this story is only partly true. Matters are complicated by the fact that not all data types in Julia can reasonably be thought of as forming a Hilbert space. e.g. the String type.","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"Consequently we introduce the special type NoTangent, instances of which can be thought of as representing the set containing only a 0 tangent. Morally speaking, for any non-differentiable data x, x + NoTangent() == x.","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"Other than non-differentiable data, the model of data in Julia as living in a real-valued finite dimensional Hilbert space is quite reasonable. Therefore, we hope readers will forgive us for largely ignoring the distinction between the domain and range of a function and that of its derivative in mathematical discussions, while simultaneously drawing a distinction when discussing code.","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"Representing Tangents","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"The extended docstring for tangent_type provides the best introduction to the types which are used to represent tangents.","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"tangent_type(P)","category":"page"},{"location":"mathematical_interpretation/#Tapir.tangent_type-Tuple{Any}","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.tangent_type","text":"tangent_type(P)\n\nThere must be a single type used to represents tangents of primals of type P, and it must be given by tangent_type(P).\n\nExtended help\n\nThe tangent types which Tapir.jl uses are quite similar in spirit to ChainRules.jl. For example, tangent \"vectors\" for\n\nFloat64s are Float64s,\nVector{Float64}s are Vector{Float64}s, and\nstructs are other another (special) struct with field types specified recursively.\n\nThere are, however, some major differences. Firstly, while it is certainly true that the above tangent types are permissible in ChainRules.jl, they are not the uniquely permissible types. For example, ZeroTangent is also a permissible type of tangent for any of them, and Float32 is permissible for Float64. This is a general theme in ChainRules.jl – it intentionally declines to place restrictions on what type can be used to represent the tangent of a given type.\n\nTapir.jl differs from this. It insists that each primal type is associated to a single tangent type. Furthermore, this type is always given by the function Tapir.tangent_type(primal_type).\n\nConsider some more worked examples.\n\nInt\n\nInt is not a differentiable type, so its tangent type is NoTangent:\n\njulia> tangent_type(Int)\nNoTangent\n\nTuples\n\nThe tangent type of a Tuple is defined recursively based on its field types. For example\n\njulia> tangent_type(Tuple{Float64, Vector{Float64}, Int})\nTuple{Float64, Vector{Float64}, NoTangent}\n\nThere is one edge case to be aware of: if all of the field of a Tuple are non-differentiable, then the tangent type is NoTangent. For example,\n\njulia> tangent_type(Tuple{Int, Int})\nNoTangent\n\nStructs\n\nAs with Tuples, the tangent type of a struct is, by default, given recursively. In particular, the tangent type of a struct type is Tangent. This type contains a NamedTuple containing the tangent to each field in the primal struct.\n\nAs with Tuples, if all field types are non-differentiable, the tangent type of the entire struct is NoTangent.\n\nThere are a couple of additional subtleties to consider over Tuples though. Firstly, not all fields of a struct have to be defined. Fortunately, Julia makes it easy to determine how many of the fields might possibly not be defined. The tangent associated to any field which might possibly not be defined is wrapped in a PossiblyUninitTangent.\n\nFurthermore, structs can have fields whose static type is abstract. For example\n\njulia> struct Foo\n           x\n       end\n\nIf you ask for the tangent type of Foo, you will see that it is\n\njulia> tangent_type(Foo)\nTangent{@NamedTuple{x}}\n\nObserve that the field type associated to x is Any. The way to understand this result is to observe that\n\nx could have literally any type at runtime, so we know nothing about what its tangent  type must be until runtime, and\nwe require that the tangent type of Foo be unique.\n\nThe consequence of these two considerations is that the tangent type of Foo must be able to contain any type of tangent in its x field. It follows that the fieldtype of the x field of Foos tangent must be Any.\n\nMutable Structs\n\nThe tangent type for mutable structs have the same set of considerations as structs. The only difference is that they must themselves be mutable. Consequently, we use a type called MutableTangent to represent their tangents. It is a mutable struct with the same structure as Tangent.\n\nFor example, if you ask for the tangent_type of\n\njulia> mutable struct Bar\n           x::Float64\n       end\n\nyou will find that it is\n\njulia> tangent_type(Bar)\nMutableTangent{@NamedTuple{x::Float64}}\n\nPrimitive Types\n\nWe've already seen a couple of primitive types (Float64 and Int). The basic story here is that all primitive types require an explicit specification of what their tangent type must be.\n\nOne interesting case are Ptr types. The tangent type of a Ptr{P} is Ptr{T}, where T = tangent_type(P). For example\n\njulia> tangent_type(Ptr{Float64})\nPtr{Float64}\n\n\n\n\n\n","category":"method"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"FData and RData","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"While tangents are the things used to represent gradients, they are not strictly what gets propagated forwards and backwards by rules during AD. Rather, they are split into fdata and rdata, and these are passed around.","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"Tapir.fdata_type(T)","category":"page"},{"location":"mathematical_interpretation/#Tapir.fdata_type-Tuple{Any}","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.fdata_type","text":"fdata_type(T)\n\nReturns the type of the forwards data associated to a tangent of type T.\n\nExtended help\n\nRules in Tapir.jl do not operate on tangents directly. Rather, functionality is defined to split each tangent into two components, that we call fdata (forwards-pass data) and rdata (reverse-pass data). In short, any component of a tangent which is identified by its address (e.g. a mutable structs or an Array) gets passed around on the forwards-pass of AD and is incremented in-place on the reverse-pass, while components of tangents identified by their value get propagated and accumulated only on the reverse-pass.\n\nGiven a tangent type T, you can find out what type its fdata and rdata must be with fdata_type(T) and rdata_type(T) respectively. A consequence of this is that there is exactly one valid fdata type and rdata type for each primal type.\n\nGiven a tangent t, you can get its fdata and rdata using f = fdata(t) and r = rdata(t) respectively. f can be re-combined to recover the original tangent using the binary version of tangent: tangent(f, r). It must always hold that\n\ntangent(fdata(t), rdata(t)) === t\n\nThe need for all of this is explained in the docs, but for now it suffices to consider our running examples again, and to see what their fdata and rdata look like.\n\nInt\n\nInts are non-differentiable types, so there is nothing to pass around on the forwards- or reverse-pass. Therefore\n\njulia> fdata_type(tangent_type(Int)), rdata_type(tangent_type(Int))\n(NoFData, NoRData)\n\nFloat64\n\nThe tangent type of Float64 is Float64. Float64s are identified by their value / have no fixed address, so\n\njulia> (fdata_type(Float64), rdata_type(Float64))\n(NoFData, Float64)\n\nVector{Float64}\n\nThe tangent type of Vector{Float64} is Vector{Float64}. A Vector{Float64} is identified by its address, so\n\njulia> (fdata_type(Vector{Float64}), rdata_type(Vector{Float64}))\n(Vector{Float64}, NoRData)\n\nTuple{Float64, Vector{Float64}, Int}\n\nThis is an example of a type which has both fdata and rdata. The tangent type for Tuple{Float64, Vector{Float64}, Int} is Tuple{Float64, Vector{Float64}, NoTangent}. Tuples have no fixed memory address, so we interogate each field on its own. We have already established the fdata and rdata types for each element, so we recurse to obtain:\n\njulia> T = tangent_type(Tuple{Float64, Vector{Float64}, Int})\nTuple{Float64, Vector{Float64}, NoTangent}\n\njulia> (fdata_type(T), rdata_type(T))\n(Tuple{NoFData, Vector{Float64}, NoFData}, Tuple{Float64, NoRData, NoRData})\n\nThe zero tangent for (5.0, [5.0]) is t = (0.0, [0.0]). fdata(t) returns (NoFData(), [0.0]), where the second element is === to the second element of t. rdata(t) returns (0.0, NoRData()). In this example, t contains a mixture of data, some of which is identified by its value, and some of which is identified by its address, so there is some fdata and some rdata.\n\nStructs\n\nStructs are handled in more-or-less the same way as Tuples, albeit with the possibility of undefined fields needing to be explicitly handled. For example, a struct such as\n\njulia> struct Foo\n           x::Float64\n           y\n           z::Int\n       end\n\nhas tangent type\n\njulia> tangent_type(Foo)\nTangent{@NamedTuple{x::Float64, y, z::NoTangent}}\n\nIts fdata and rdata are given by special FData and RData types:\n\njulia> (fdata_type(tangent_type(Foo)), rdata_type(tangent_type(Foo)))\n(Tapir.FData{@NamedTuple{x::NoFData, y, z::NoFData}}, Tapir.RData{@NamedTuple{x::Float64, y, z::NoRData}})\n\nPractically speaking, FData and RData both have the same structure as Tangents and are just used in different contexts.\n\nMutable Structs\n\nThe fdata for a mutable structs is its tangent, and it has no rdata. This is because mutable structs have fixed memory addresses, and can therefore be incremented in-place. For example,\n\njulia> mutable struct Bar\n           x::Float64\n           y\n           z::Int\n       end\n\nhas tangent type\n\njulia> tangent_type(Bar)\nMutableTangent{@NamedTuple{x::Float64, y, z::NoTangent}}\n\nand fdata / rdata types\n\njulia> (fdata_type(tangent_type(Bar)), rdata_type(tangent_type(Bar)))\n(MutableTangent{@NamedTuple{x::Float64, y, z::NoTangent}}, NoRData)\n\nPrimitive Types\n\nAs with tangents, each primitive type must specify what its fdata and rdata is. See specific examples for details.\n\n\n\n\n\n","category":"method"},{"location":"mathematical_interpretation/#The-Rule-Abstraction","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"The Rule Abstraction","text":"","category":"section"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"A rule must return a CoDual and a function to run the reverse-pass, known as the pullback. Upon exit from the rule, it must be true that","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"the state of the arguments / output are the same as they would be had the primal been run, and\nthe uniqueness of the mapping between address-identified primals and their fdata is maintained.","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"Upon exit from the pullback, it must be true that","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"the primal state is as it was before running the rule,\nthe fdata for the arguments has been incremented by the fdata in D fx^ast (bary), and \nthe rdata for the arguments is equal to the rdata in D fx^ast (bary).","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"Tapir.jl makes use of a rule system which is at first glance similar to the rrule function offered by ChainRules.jl. However, owing to Tapir.jl's support for mutation (e.g. differentiating through functions which write to arrays) and high degree of precision around the types used to represent (co)-tangent-like data, the number of situations in which the two are identical are actually rather small.","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"Nevertheless, we begin this explanation with an example which should be familiar to anyone who has used ChainRules.jl and seen its rrule. Once this example has been explained, we move into new territory.","category":"page"},{"location":"mathematical_interpretation/#Functions-of-Scalars:-from-ChainRules.rrule-to-Tapir.rrule!!","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Functions of Scalars: from ChainRules.rrule to Tapir.rrule!!","text":"","category":"section"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"Consider the simple Julia function","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"mul(a::Float64, b::Float64) = a * b","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"A ChainRules.rrule for this might look something like","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"function ChainRules.rrule(::typeof(mul), a::Float64, b::Float64)\n    mul_pullback(dc::Float64) = NoTangent(), dc * b, dc * a\n    return a * b, mul_pullback\nend","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"The corresponding Tapir.rrule!! would be something like","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"function Tapir.rrule!!(::CoDual{typeof(mul)}, a::CoDual{Float64}, b::CoDual{Float64})\n    _a = primal(a)\n    _b = primal(b)\n    mul_pullback!!(dc::Float64) = NoRData(), dc * _b, dc * _a\n    return CoDual(_a * _b, NoFData()), mul_pullback!\nend","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"The core differences between the rrule and rrule!! are:","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"each argument is a CoDual, which contains the primal and one other piece of data (more on this later),\nwe must extract the primal values from a and b using the primal function in order to access them,\nNoTangent() is replaced by NoRData(), and\nwe must return another CoDual, rather than just the primal value (more on this later).","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"The point of this example is to highlight that Tapir.rrule!!s look a lot like ChainRules.rrules in some situations, so some of your existing knowledge should transfer over.","category":"page"},{"location":"mathematical_interpretation/#Functions-of-Vectors","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Functions of Vectors","text":"","category":"section"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"We now turn to the obvious question: why do Tapir.rrule!!s differ from ChainRules.rrules? The short answer is that Tapir.jl requires that each unique primal memory address associated to differentiable data be associated to a unique tangent (a.k.a. shadow) memory address. (See Why Unique Memory Address to understand why this is necessary.)","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"To see how this is achieved, consider the function","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"function set_1!(x::Vector{Float64}, y::Float64)\n    x[1] = y\n    return x\nend","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"A valid Tapir.rrule!! for this function given below. There are a lot of concepts introduced here, so you'll need to hop back and forth between this and the text below which explains everything.","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"function Tapir.rrule!!(\n    ::CoDual{typeof(set_1!)}, x::CoDual{Vector{Float64}}, y::CoDual{Float64}\n)\n    # Extract the primal and \"fdata\" from x.\n    px = primal(x)\n    dx = tangent(x)\n\n    # Store the current values.\n    px_1_old = px[1]\n    dx_1_old = dx[1]\n\n    # Set x_p[1] to `y` and zero-out x_f[1].\n    px[1] = primal(y)\n    dx[1] = 0.0\n\n    function set_1_pullback!!(::NoRData)\n\n        # The (co)tangent to `y` is just the value in the first position of x_f.\n        dy = dx\n\n        # We _must_ undo any changes which occur on the forwards-pass, both to the primal\n        # and the fdata (the forwards-component of the tangent).\n        px[1] = px_1_old\n        dx[1] = dx_1_old\n\n        # There's nothing to propagate backwards for `f` because it's non-differentiable.\n        # It has \"no reverse data\", hence `NoRData`.\n        df = NoRData()\n\n        # There's nothing to propagate backwards for `x`, because its tangent is entirely\n        # represented by `dx` on the forwards-pass, hence `NoRData`.\n        dx = NoRData()\n\n        return df, dx, dy\n    end\n\n    # Just return x (the CoDual) -- this propagates forwards the correct unique tangent\n    # memory for `x`.\n    return x, set_1_pullback!!\nend","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"Let's unpack the above:","category":"page"},{"location":"mathematical_interpretation/#Memory-Propagation","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Memory Propagation","text":"","category":"section"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"We stated at the top of this section that each unique address associated to differentiable data must have a unique tangent memory address associated to it. To see how this rule preserves this, consider the function","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"g(x::Vector{Float64}, y::Float64) = x, set_1!(x, y)","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"The output of g is a Tuple with the same Vector{Float64} in each element. Therefore, during AD, they must be associated to the same tangent address. Happily, simple by by returning x at the end of the rrule!! for set_1! we ensure that this happens.","category":"page"},{"location":"mathematical_interpretation/#The-other-field-in-a-CoDual","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"The other field in a CoDual","text":"","category":"section"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"In this example, the other field in the CoDual associated to x must contain a Vector{Float64}, which represents the tangent to x. We call this the fdata (\"forwards data\") associated to x. We didn't show it, but the fdata associated to y is NoFData (\"no forwards data\"), indicating that there is no additional data associated to y on the forwards-pass.","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"Why is this the case?","category":"page"},{"location":"mathematical_interpretation/#Summary","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Summary","text":"","category":"section"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"Note that this very simple function does not have a meaningful ChainRules.rrule counterpart because it mutates (modifies) x, and ChainRules.rrule does not support mutation.","category":"page"},{"location":"mathematical_interpretation/#Asides","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Asides","text":"","category":"section"},{"location":"mathematical_interpretation/#Why-Uniqueness-of-Type-For-Tangents-/-FData-/-RData?","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Why Uniqueness of Type For Tangents / FData / RData?","text":"","category":"section"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"Why does Tapir.jl insist that each primal type P be paired with a single tangent type T, as opposed to being more permissive. There are a few notable reasons:","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"To provide a precise interface. Rules pass fdata around on the forwards-pass and rdata on the reverse-pass – being able to make strong assumptions about the type of the fdata / rdata given the primal type makes implementing rules much easier in practice.\nConditional type stability. We wish to have a high degree of confidence that if the primal code is type-stable, then the AD code will also be. It is straightforward to construct type stable primal codes which have type-unstable forwards- and reverse-passes if you permit there to be more than one fdata / rdata type for a given primal. So while uniqueness is certainly not sufficient on its own to guarantee conditional type stability, it is probably necessary in general.\nTest-case generation and coverage. There being a unique tangent / fdata / rdata type for each primal makes being confident that a given rule is being tested thoroughly much easier. For a given primal, rather than there being many possible input / output types to consider, there is just one.","category":"page"},{"location":"mathematical_interpretation/","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Tapir.jl's Mathematical Interpration of Julia Functions","text":"This topic, in particular what goes wrong with permissive tangent type systems like those employed by ChainRules, deserves a more thorough treatment – hopefully someone will write something more expansive on this topic at some point.","category":"page"},{"location":"mathematical_interpretation/#Why-Unique-Memory-Address","page":"Tapir.jl's Mathematical Interpration of Julia Functions","title":"Why Unique Memory Address","text":"","category":"section"}]
}
