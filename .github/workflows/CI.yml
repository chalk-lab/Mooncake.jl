name: CI
on:
  push:
    branches:
      - main
    tags: ['*']
  pull_request:
concurrency:
  # Skip intermediate builds: always.
  # Cancel intermediate builds: only if it is a pull request build.
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ startsWith(github.ref, 'refs/pull/') }}
jobs:
  # tests:
  #   name: test-${{ matrix.test_group }}
  #   runs-on: ubuntu-latest
  #   strategy:
  #     fail-fast: false
  #     matrix:
  #       test_group:
  #         # - 'basic'
  #         # - 'integration_testing/misc'
  #         # - 'integration_testing/diff_tests'
  #         # - 'integration_testing/distributions'
  #         - 'integration_testing/special_functions'
  #         - 'foo'
  #         # - 'integration_testing/array'
  #         # - 'integration_testing/turing'
  #   steps:
  #     - uses: actions/checkout@v4
  #     - uses: julia-actions/setup-julia@v1
  #       with:
  #         version: 1.9
  #         arch: x64
  #     - uses: julia-actions/cache@v1
  #     - uses: julia-actions/julia-buildpkg@v1
  #     - name: Set start time
  #       id: start-time
  #       run: echo "start_time=$(date +%s)" >> "$GITHUB_ENV"
  #     # - uses: julia-actions/julia-runtest@v1
  #     #   env:
  #     #     TEST_GROUP: ${{ matrix.test_group }}
  #     - name: Set end time
  #       id: end-time
  #       run: echo "end_time=$(date +%s)" >> "$GITHUB_ENV"
  #     - name: Calculate elapsed time
  #       id: elapsed-time
  #       run: echo "elapsed_time=$((end_time - start_time))" >> "$GITHUB_ENV"
  #     - name: Log elapsed time
  #       run: echo "Job took $elapsed_time seconds"
  #     # - name: Log Results
  #     #   run: |
  #     #     using Pkg
  #     #     Pkg.activate(; temp=true)
  #     #     Pkg.add(["DataFrames", "PythonCall", "Plots", "Wandb"])
  #     #   shell: julia --color=yes {0}
  perf:
    name: "Performance Analysis"
    # needs: tests
    runs-on: ubuntu-latest
    steps:
      # - name: Visualise Results
      #   run: |
      #     using Pkg
      #     Pkg.activate(; temp=true)
      #     Pkg.add(["DataFrames", "PythonCall", "Plots", "Wandb"])
      #     using DataFrames, PythonCall, Plots, Wandb
      #     api = Wandb.wandb.Api()
      #     run = api.run("willtebbutt/my-awesome-project/extensible-run-id")
      #     df = DataFrame(map(x -> pyconvert(Dict, x), run.history()))
      #     savefig(plot(randn(10)), "perf.png")
      #   env:
      #     WANDB_API_KEY: ${{secrets.WANDB_TOKEN}}
      #     GKSwstype: "100" # run GK (plots backend) in headless mode
      #   shell: julia --color=yes {0}
      # - name: Upload artifact
      #   uses: actions/upload-artifact@v3
      #   with:
      #     name: benchmarking-results
      #     path: perf.png
      - name: Comment on PR
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const prNumber = context.payload.pull_request.number;
            const runId = process.env.GITHUB_RUN_ID;
            const commentBody = 'Benchmarking complete. Results can be found in artifact' +
              '[here](https://github.com/withbayes/Taped.jl/actions/runs/' + runId + ').';
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber,
              body: commentBody,
            });
