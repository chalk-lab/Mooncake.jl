name: CI
on:
  push:
    branches:
      - main
    tags: ['*']
  pull_request:
concurrency:
  # Skip intermediate builds: always.
  # Cancel intermediate builds: only if it is a pull request build.
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ startsWith(github.ref, 'refs/pull/') }}
jobs:
  tests:
    name: test-${{ matrix.test_group }}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        test_group:
          # - 'basic'
          # - 'integration_testing/misc'
          # - 'integration_testing/diff_tests'
          # - 'integration_testing/distributions'
          - 'integration_testing/special_functions'
          - 'foo'
          # - 'integration_testing/array'
          # - 'integration_testing/turing'
    steps:
      - uses: actions/checkout@v4
      - uses: julia-actions/setup-julia@v1
        with:
          version: 1.9
          arch: x64
      - uses: julia-actions/cache@v1
      - uses: julia-actions/julia-buildpkg@v1
      - name: Set start time
        id: start-time
        run: echo "start_time=$(date +%s)" >> "$GITHUB_ENV"
      # - uses: julia-actions/julia-runtest@v1
      #   env:
      #     TEST_GROUP: ${{ matrix.test_group }}
      - name: Set end time
        id: end-time
        run: echo "end_time=$(date +%s)" >> "$GITHUB_ENV"
      - name: Calculate elapsed time
        id: elapsed-time
        run: echo "elapsed_time=$((end_time - start_time))" >> "$GITHUB_ENV"
      - name: Log elapsed time
        run: echo "Job took $elapsed_time seconds"
      - name: Log Results
        run: |
          using Pkg
          Pkg.activate(; temp=true)
          Pkg.add("Wandb")
          using Wandb
          lg = WandbLogger(
              project = "taped-benchmarking", name="runtime", id="runtime", resume=true,
          )
          Wandb.log(lg, Dict("runtime" => 1.5, "test_group" => ENV["TEST_GROUP"]))
          close(lg)
        shell: julia --color=yes {0}
        env:
          TEST_GROUP: ${{ matrix.test_group }}
          WANDB_API_KEY: ${{secrets.WANDB_TOKEN}}
  perf:
    name: "Performance Analysis"
    needs: tests
    runs-on: ubuntu-latest
    steps:

      # Pulls down results from Wandb, and generates a plot from them.
      - name: Visualise Results
        run: |
          using Pkg
          Pkg.activate(; temp=true)
          Pkg.add(["DataFrames", "PythonCall", "Plots", "Wandb"])
          using DataFrames, PythonCall, Plots, Wandb
          api = Wandb.wandb.Api()
          run = api.run("willtebbutt/my-awesome-project/extensible-run-id")
          df = DataFrame(map(x -> pyconvert(Dict, x), run.history()))
          savefig(plot(randn(10)), "perf.png")
        env:
          GKSwstype: "100" # run GK (plots backend) in headless mode
          WANDB_API_KEY: ${{secrets.WANDB_TOKEN}}
        shell: julia --color=yes {0}

      # Stores benchmarking plots as an artifact.
      - name: Upload artifact
        uses: actions/upload-artifact@v3
        with:
          name: benchmarking-results
          path: perf.png

      # Comments on the PR which triggered this action with a link to the results.
      - name: Comment on PR
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const prNumber = context.payload.pull_request.number;
            const runId = process.env.GITHUB_RUN_ID;
            const commentBody = 'Benchmarking complete. Results can be found in artifact ' +
              '[here](https://github.com/withbayes/Taped.jl/actions/runs/' + runId + ').';
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber,
              body: commentBody,
            });
